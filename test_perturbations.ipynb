{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model Architectures\n",
    "Custom Torch Models need to be instantiated for evaluation. The model_architectures.py file contains the model architectures so we can abstract it and focus only on the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "['BasicBlock', 'DataLoader', 'F', 'Load', 'ResNetCIFAR', 'ResNetMNIST', 'ResnetSVHN', 'TensorDataset', 'Tester', 'Visualizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'cifar_directory', 'conv3x3', 'current_directory', 'device', 'mnist_directory', 'nn', 'np', 'os', 'pd', 'pickle', 'plt', 'scipy', 'svhn_directory', 'torch']\n"
     ]
    }
   ],
   "source": [
    "from model_architectures import *\n",
    "import model_architectures\n",
    "from attacks import * \n",
    "# Print available classes to verify our model architectures were imported\n",
    "print(dir(model_architectures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model Weights\n",
    "Using our model artifacts we load the weights back into the model so we have our pre-trained models to test our perturbations against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_resnet_model = ResNetMNIST(BasicBlock, [2, 2, 2, 2], num_classes=10, grayscale=True).to(device)\n",
    "cifar_resnet_model = ResNetCIFAR(BasicBlock, [2, 2, 2, 2], num_classes=10, grayscale=False).to(device)\n",
    "svhn_resnet_model = ResnetSVHN(BasicBlock, [2, 2, 2, 2], num_classes=10, grayscale=False).to(device)\n",
    "\n",
    "mnist_resnet_model.load_state_dict(torch.load(\"artifacts/resnet18_mnist_model.pth\"))\n",
    "cifar_resnet_model.load_state_dict(torch.load(\"artifacts/resnet18_cifar_model.pth\"))\n",
    "svhn_resnet_model.load_state_dict(torch.load(\"artifacts/resnet18_svhn_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetSVHN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set models to evaluation mode\n",
    "mnist_resnet_model.eval()\n",
    "cifar_resnet_model.eval()\n",
    "svhn_resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading/Visualizing Data\n",
    "Functionality to load the test dataset and labels as numpy arrays and visualize any given image from the numpy array has been implemented in the model_architectures.py file for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Load()\n",
    "mnist_test_images, mnist_test_labels = loader.load_mnist_test_images()\n",
    "cifar10_test_images, cifar10_test_labels = loader.load_cifar10_test_images()\n",
    "svhn_test_images, svhn_test_labels = loader.load_svhn_test_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visualizer with an image available in test data\n",
    "viz = Visualizer()\n",
    "# MNIST\n",
    "i = 1\n",
    "print(f\"MNIST Shape: {mnist_test_images[i].shape}\")\n",
    "print(f\"MNIST Label: {mnist_test_labels[i]}\")\n",
    "viz.show(mnist_test_images[i])\n",
    "\n",
    "# CIFAR-10\n",
    "label_mapping = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "print(f\"\\nCIFAR-10 Shape: {cifar10_test_images[i].shape}\")\n",
    "print(f\"CIFAR-10 Label: {label_mapping[cifar10_test_labels[i]]}\")\n",
    "viz.show(cifar10_test_images[i])\n",
    "\n",
    "# SVHN\n",
    "print(f\"\\nSVHN Shape: {svhn_test_images[i].shape}\")\n",
    "print(f\"SVHN Label: {svhn_test_labels[i]}\")\n",
    "viz.show(svhn_test_images[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Models (Unperturbed Data)\n",
    "The data is converted to PyTorch tensors and loaded with a Data Loader for the model to be evaluated. The model can only take in Data Loaders to iterate through the data so after perturbations, we have to load it with the data loader and then evaluate the model. We can verify our models by evaluating the clean test sets and checking the accuracy is equal to our expected accuracies: 99% for MNIST, 76% for CIFAR10, and 93% for SVHN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_mnist = loader.convert_mnist_numpy_to_tensor(mnist_test_images[:256], mnist_test_labels[:256])\n",
    "test_loader_cifar10 = loader.convert_cifar10_numpy_to_tensor(cifar10_test_images[:256], cifar10_test_labels[:256])\n",
    "test_loader_svhn = loader.convert_svhn_numpy_to_tensor(svhn_test_images[:256], svhn_test_labels[:256])\n",
    "tester = Tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mnist = tester.test(mnist_resnet_model, test_loader_mnist)\n",
    "print(f'Test Accuracy MNIST: {acc_mnist * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cifar10 = tester.test(cifar_resnet_model, test_loader_cifar10)\n",
    "print(f'Test Accuracy CIFAR10: {acc_cifar10 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_svhn = tester.test(svhn_resnet_model, test_loader_svhn)\n",
    "print(f'Test Accuracy SVHN: {acc_svhn * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbing an Image and Testing Accuracy\n",
    "As a simple test we'll just flip the image so it's reversed. In this process we use perturb to modify the images and then reload it with a Data Loader and test it against our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flip(images):\n",
    "    \"\"\"\n",
    "    Flip images along the specified axis.\n",
    "\n",
    "    Parameters:\n",
    "    - images: numpy array with shape (num_images, channels, height, width)\n",
    "    - axis: Axis along which to flip the images (0 for vertical, 1 for horizontal)\n",
    "\n",
    "    Returns:\n",
    "    - Perturbed images\n",
    "    \"\"\"\n",
    "    flip_axis = 1\n",
    "\n",
    "    perturbed_images = np.empty_like(images)\n",
    "    for i in range(images.shape[0]):\n",
    "        perturbed_image = np.flip(images[i, 0, :, :], axis=flip_axis)\n",
    "        perturbed_images[i, 0, :, :] = perturbed_image\n",
    "    return perturbed_images\n",
    "\n",
    "\n",
    "flipped_images_array = test_flip(mnist_test_images)\n",
    "# Show example of the image after being flipped\n",
    "viz.show(flipped_images_array[1])\n",
    "\n",
    "flipped_images_tensor = loader.convert_mnist_numpy_to_tensor(flipped_images_array, mnist_test_labels)\n",
    "acc_mnist_flipped = tester.test(mnist_resnet_model, flipped_images_tensor)\n",
    "print(f'Test Accuracy MNIST (Flipped): {acc_mnist_flipped * 100:.2f}%')\n",
    "# Accuracy drops 50% but the image is obviously the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient Sign Method\n",
    "The ``fgsm_attack`` function takes three\n",
    "inputs, *image* is the original clean image ($x$), *epsilon* is\n",
    "the pixel-wise perturbation amount ($\\epsilon$), and *data_grad*\n",
    "is gradient of the loss w.r.t the input image\n",
    "($\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)$). The function\n",
    "then creates perturbed image as\n",
    "\n",
    "\\begin{align}\n",
    "\\text{perturbed\\_image} &= \\text{image} + \\epsilon \\cdot \\text{sign}(\\text{data\\_grad}) \\\\\n",
    "&x = x + \\epsilon \\cdot \\text{sign}(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\n",
    "\\end{align}\n",
    "\n",
    "Fast Gradient Sign Method uses $L^∞$ Norm Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fgsm(model, test_loader, epsilon):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    adv_examples = []\n",
    "    batch = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch += 1\n",
    "        print(f\"Batch: {batch}, Epsilon: {epsilon}, Correct: {correct}\")\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            label = label.unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            output, _ = model(image)\n",
    "\n",
    "            # print(outputs)\n",
    "\n",
    "            _, init_pred = torch.max(output.data, 1)\n",
    "\n",
    "            if not torch.equal(init_pred, label):\n",
    "                total +=1 \n",
    "                continue\n",
    "            \n",
    "            loss = F.nll_loss(output, label)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = image.grad.data\n",
    "            perturbed_data = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "            output_final, _ = model(perturbed_data)\n",
    "            _, final_pred = torch.max(output_final.data, 1)\n",
    "            if torch.equal(final_pred, label):\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            else:\n",
    "                # Save some adv examples for visualization later\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            total +=1 \n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {total} = {accuracy}\")\n",
    "    return accuracy, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .5]\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test_fgsm(mnist_resnet_model, test_loader_mnist, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .55, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(f\"{orig} -> {adv}\")\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFool\n",
    "The `deepfool_attack` function is designed to generate adversarial examples using the DeepFool algorithm. It iteratively picks the classified label closest to the true label that isn't the true label and then uses the gradient to move the image closer to that classification boundary while retaining image data as much as possible. It is considered a \"perfect\" algorithm as it terminates when the image misclassifies but we limit resources because this could take unreasonable amounts of time in theory. \n",
    "\n",
    "\\begin{array}{l}\n",
    "\\textbf{while } sign(f(x_i)) = sign(f(x_0)): \\\\\n",
    "\\hspace{2em} r_i = - \\frac{f(x_i)}{\\|\\nabla f(x_i)\\|_2^2} \\cdot \\nabla f(x) \\\\\n",
    "\\hspace{2em} x_{i+1} = x_i + r_i \\\\\n",
    "\\hspace{2em} i = i + 1\n",
    "\\end{array}\n",
    "\n",
    "Deepfool uses $L^2$ Norm Perturbations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deepfool(model, test_loader, overshoot=0.02):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    adv_examples = []\n",
    "    batch = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch += 1\n",
    "        print(f\"Batch: {batch}, Correct: {correct}\")\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            label = label.unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            output, _ = model(image)\n",
    "\n",
    "            # print(outputs)\n",
    "\n",
    "            _, init_pred = torch.max(output.data, 1)\n",
    "\n",
    "            if not torch.equal(init_pred, label):\n",
    "                total +=1 \n",
    "                continue\n",
    "            \n",
    "            perturbed_image, final_pred, r_total, iter = deepfool_attack(image, model, overshoot=0.02, max_iterations=100)\n",
    "            print(f\"Perturbed Iteration: {iter}\")\n",
    "            if torch.equal(final_pred, label):\n",
    "                correct += 1\n",
    "            total +=1 \n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy = {correct} / {total} = {accuracy}\")\n",
    "    return accuracy, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_deepfool(cifar_resnet_model, test_loader_cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent\n",
    "Projected Gradient Descent is an optimization algorithm that iteratively computes the gradient of the loss and perturbs the image in the direction of the loss gradient. This algorithm has unique hyperparameters. We have a value $α$ which controls the step size the image moves along the gradient, but we also have something known as the $ϵ$-ball. Around the image in the vector space exists a multi dimensional threshold where the image is still percievable to humans. We clip the perturbed output image such that it always exists within this epsilon ball, and we can expand or shrink the epsilon to tune results.\n",
    "\n",
    "\\begin{array}{l}\n",
    "\\textbf{while } t < \\text{ iterations:} \\\\\n",
    "\\hspace{2em} x_{t+1} = Clip(x_t + \\alpha \\cdot sign(\\nabla_x J(\\theta, x_t, y)), x, x + \\epsilon)\n",
    "\\end{array}\n",
    "\n",
    "PGA uses uses $L^∞$ Norm Perturbations and MUST be used against  classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pgd(model, test_loader, epsilon, alpha):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    adv_examples = []\n",
    "    batch = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch += 1\n",
    "        print(f\"Batch: {batch}, Epsilon: {epsilon}, Correct: {correct}\")\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            label = label.unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            output, _ = model(image)\n",
    "\n",
    "            _, init_pred = torch.max(output.data, 1)\n",
    "\n",
    "            if not torch.equal(init_pred, label):\n",
    "                total +=1 \n",
    "                continue\n",
    "            \n",
    "            \n",
    "\n",
    "            output_final, perturbed_data = pgd_attack(image, model, init_pred, epsilon, alpha)\n",
    "            _, final_pred = torch.max(output_final.data, 1)\n",
    "            if torch.equal(final_pred, label):\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            else:\n",
    "                # Save some adv examples for visualization later\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            print(f\"{correct}/{total}\")\n",
    "            total +=1 \n",
    "            # break\n",
    "        # break\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {total} = {accuracy}\")\n",
    "    return accuracy, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Epsilon: 8, Correct: 0\n",
      "0/0\n",
      "1/1\n",
      "1/2\n",
      "2/3\n",
      "2/4\n",
      "2/5\n",
      "3/6\n",
      "3/7\n",
      "3/8\n",
      "4/9\n",
      "4/10\n",
      "4/11\n",
      "5/12\n",
      "6/13\n",
      "7/14\n",
      "7/15\n",
      "7/16\n",
      "8/17\n",
      "8/18\n",
      "9/19\n",
      "9/20\n",
      "9/21\n",
      "10/22\n",
      "10/23\n",
      "10/24\n",
      "10/25\n",
      "10/26\n",
      "11/27\n",
      "11/28\n",
      "11/29\n",
      "12/30\n",
      "13/31\n",
      "13/32\n",
      "13/33\n",
      "14/34\n",
      "15/35\n",
      "15/36\n",
      "16/37\n",
      "16/38\n",
      "16/39\n",
      "17/40\n",
      "17/41\n",
      "18/42\n",
      "18/43\n",
      "19/44\n",
      "20/45\n",
      "20/46\n",
      "21/47\n",
      "21/48\n",
      "22/49\n",
      "23/50\n",
      "23/51\n",
      "24/52\n",
      "25/53\n",
      "26/54\n",
      "27/55\n",
      "28/56\n",
      "28/57\n",
      "28/58\n",
      "28/59\n",
      "29/60\n",
      "29/61\n",
      "29/62\n",
      "29/63\n",
      "29/64\n",
      "29/65\n",
      "30/66\n",
      "30/67\n",
      "31/68\n",
      "32/69\n",
      "32/70\n",
      "33/71\n",
      "34/72\n",
      "34/73\n",
      "35/74\n",
      "36/75\n",
      "37/76\n",
      "38/77\n",
      "38/78\n",
      "38/79\n",
      "38/80\n",
      "38/81\n",
      "39/82\n",
      "40/83\n",
      "41/84\n",
      "42/85\n",
      "43/86\n",
      "43/87\n",
      "43/88\n",
      "43/89\n",
      "44/90\n",
      "44/91\n",
      "44/92\n",
      "44/93\n",
      "45/94\n",
      "45/95\n",
      "45/96\n",
      "46/97\n",
      "46/98\n",
      "47/99\n",
      "47/100\n",
      "48/101\n",
      "48/102\n",
      "49/103\n",
      "49/104\n",
      "50/105\n",
      "51/106\n",
      "51/107\n",
      "52/108\n",
      "53/109\n",
      "53/110\n",
      "53/111\n",
      "54/112\n",
      "55/113\n",
      "56/114\n",
      "56/115\n",
      "57/116\n",
      "58/117\n",
      "58/118\n",
      "58/119\n",
      "58/120\n",
      "58/121\n",
      "59/122\n",
      "60/123\n",
      "60/124\n",
      "60/125\n",
      "61/126\n",
      "62/127\n",
      "62/128\n",
      "63/129\n",
      "64/130\n",
      "64/131\n",
      "64/132\n",
      "64/133\n",
      "64/134\n",
      "64/135\n",
      "65/136\n",
      "65/137\n",
      "65/138\n",
      "65/139\n",
      "65/140\n",
      "66/141\n",
      "66/142\n",
      "67/143\n",
      "67/144\n",
      "68/145\n",
      "68/146\n",
      "69/147\n",
      "70/148\n",
      "70/149\n",
      "71/150\n",
      "71/151\n",
      "71/152\n",
      "72/153\n",
      "73/154\n",
      "74/155\n",
      "75/156\n",
      "76/157\n",
      "76/158\n",
      "76/159\n",
      "77/160\n",
      "77/161\n",
      "78/162\n",
      "79/163\n",
      "79/164\n",
      "80/165\n",
      "81/166\n",
      "81/167\n",
      "82/168\n",
      "83/169\n",
      "84/170\n",
      "84/171\n",
      "84/172\n",
      "84/173\n",
      "84/174\n",
      "84/175\n",
      "85/176\n",
      "85/177\n",
      "85/178\n",
      "85/179\n",
      "86/180\n",
      "87/181\n",
      "87/182\n",
      "88/183\n",
      "88/184\n",
      "88/185\n",
      "89/186\n",
      "89/187\n",
      "90/188\n",
      "91/189\n",
      "91/190\n",
      "91/191\n",
      "92/192\n",
      "92/193\n",
      "92/194\n",
      "92/195\n",
      "93/196\n",
      "94/197\n",
      "95/198\n",
      "95/199\n",
      "95/200\n",
      "96/201\n",
      "96/202\n",
      "96/203\n",
      "96/204\n",
      "97/205\n",
      "98/206\n",
      "99/207\n",
      "100/208\n",
      "100/209\n",
      "101/210\n",
      "101/211\n",
      "102/212\n",
      "103/213\n",
      "103/214\n",
      "104/215\n",
      "105/216\n",
      "105/217\n",
      "106/218\n",
      "106/219\n",
      "107/220\n",
      "107/221\n",
      "108/222\n",
      "108/223\n",
      "108/224\n",
      "108/225\n",
      "108/226\n",
      "108/227\n",
      "108/228\n",
      "108/229\n",
      "109/230\n",
      "110/231\n",
      "110/232\n",
      "111/233\n",
      "112/234\n",
      "112/235\n",
      "113/236\n",
      "114/237\n",
      "115/238\n",
      "116/239\n",
      "116/240\n",
      "116/241\n",
      "117/242\n",
      "118/243\n",
      "119/244\n",
      "119/245\n",
      "120/246\n",
      "120/248\n",
      "121/249\n",
      "121/250\n",
      "121/251\n",
      "122/252\n",
      "122/253\n",
      "122/254\n",
      "122/255\n",
      "Epsilon: 8\tTest Accuracy = 122 / 256 = 0.4765625\n",
      "0.4765625\n"
     ]
    }
   ],
   "source": [
    "accuracy, examples = test_pgd(mnist_resnet_model, test_loader_mnist, 8, 0.05 )\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_cols = len(examples)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_cols, figsize=(15, 5))  # Adjust figsize as needed\n",
    "for i in range(num_cols):\n",
    "    true_label, false_label, image_data = examples[i]\n",
    "    axes[i].imshow(image_data, cmap='gray')  # Assuming the image is grayscale, adjust cmap as needed\n",
    "    title = f\"{true_label} -> {false_label}\"\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
