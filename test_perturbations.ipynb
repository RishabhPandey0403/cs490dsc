{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model Architectures\n",
    "Custom Torch Models need to be instantiated for evaluation. The model_architectures.py file contains the model architectures so we can abstract it and focus only on the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "['BasicBlock', 'DataLoader', 'F', 'Load', 'ResNetCIFAR', 'ResNetMNIST', 'ResnetSVHN', 'TensorDataset', 'Tester', 'Visualizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'cifar_directory', 'conv3x3', 'current_directory', 'device', 'mnist_directory', 'nn', 'np', 'os', 'pd', 'pickle', 'plt', 'scipy', 'svhn_directory', 'torch']\n"
     ]
    }
   ],
   "source": [
    "from model_architectures import *\n",
    "import model_architectures\n",
    "from attacks import * \n",
    "# Print available classes to verify our model architectures were imported\n",
    "print(dir(model_architectures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model Weights\n",
    "Using our model artifacts we load the weights back into the model so we have our pre-trained models to test our perturbations against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_resnet_model = ResNetMNIST(BasicBlock, [2, 2, 2, 2], num_classes=10, grayscale=True).to(device)\n",
    "cifar_resnet_model = ResNetCIFAR(BasicBlock, [2, 2, 2, 2], num_classes=10, grayscale=False).to(device)\n",
    "svhn_resnet_model = ResnetSVHN(BasicBlock, [2, 2, 2, 2], num_classes=10, grayscale=False).to(device)\n",
    "\n",
    "mnist_resnet_model.load_state_dict(torch.load(\"artifacts/resnet18_mnist_model.pth\"))\n",
    "cifar_resnet_model.load_state_dict(torch.load(\"artifacts/resnet18_cifar_model.pth\"))\n",
    "svhn_resnet_model.load_state_dict(torch.load(\"artifacts/resnet18_svhn_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetSVHN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set models to evaluation mode\n",
    "mnist_resnet_model.eval()\n",
    "cifar_resnet_model.eval()\n",
    "svhn_resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading/Visualizing Data\n",
    "Functionality to load the test dataset and labels as numpy arrays and visualize any given image from the numpy array has been implemented in the model_architectures.py file for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Load()\n",
    "mnist_test_images, mnist_test_labels = loader.load_mnist_test_images()\n",
    "cifar10_test_images, cifar10_test_labels = loader.load_cifar10_test_images()\n",
    "svhn_test_images, svhn_test_labels = loader.load_svhn_test_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visualizer with an image available in test data\n",
    "viz = Visualizer()\n",
    "# MNIST\n",
    "i = 1\n",
    "print(f\"MNIST Shape: {mnist_test_images[i].shape}\")\n",
    "print(f\"MNIST Label: {mnist_test_labels[i]}\")\n",
    "viz.show(mnist_test_images[i])\n",
    "\n",
    "# CIFAR-10\n",
    "label_mapping = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "print(f\"\\nCIFAR-10 Shape: {cifar10_test_images[i].shape}\")\n",
    "print(f\"CIFAR-10 Label: {label_mapping[cifar10_test_labels[i]]}\")\n",
    "viz.show(cifar10_test_images[i])\n",
    "\n",
    "# SVHN\n",
    "print(f\"\\nSVHN Shape: {svhn_test_images[i].shape}\")\n",
    "print(f\"SVHN Label: {svhn_test_labels[i]}\")\n",
    "viz.show(svhn_test_images[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Models (Unperturbed Data)\n",
    "The data is converted to PyTorch tensors and loaded with a Data Loader for the model to be evaluated. The model can only take in Data Loaders to iterate through the data so after perturbations, we have to load it with the data loader and then evaluate the model. We can verify our models by evaluating the clean test sets and checking the accuracy is equal to our expected accuracies: 99% for MNIST, 76% for CIFAR10, and 93% for SVHN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_mnist = loader.convert_mnist_numpy_to_tensor(mnist_test_images[:256], mnist_test_labels[:256])\n",
    "test_loader_cifar10 = loader.convert_cifar10_numpy_to_tensor(cifar10_test_images[:256], cifar10_test_labels[:256])\n",
    "test_loader_svhn = loader.convert_svhn_numpy_to_tensor(svhn_test_images[:256], svhn_test_labels[:256])\n",
    "tester = Tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mnist = tester.test(mnist_resnet_model, test_loader_mnist)\n",
    "print(f'Test Accuracy MNIST: {acc_mnist * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cifar10 = tester.test(cifar_resnet_model, test_loader_cifar10)\n",
    "print(f'Test Accuracy CIFAR10: {acc_cifar10 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_svhn = tester.test(svhn_resnet_model, test_loader_svhn)\n",
    "print(f'Test Accuracy SVHN: {acc_svhn * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbing an Image and Testing Accuracy\n",
    "As a simple test we'll just flip the image so it's reversed. In this process we use perturb to modify the images and then reload it with a Data Loader and test it against our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flip(images):\n",
    "    \"\"\"\n",
    "    Flip images along the specified axis.\n",
    "\n",
    "    Parameters:\n",
    "    - images: numpy array with shape (num_images, channels, height, width)\n",
    "    - axis: Axis along which to flip the images (0 for vertical, 1 for horizontal)\n",
    "\n",
    "    Returns:\n",
    "    - Perturbed images\n",
    "    \"\"\"\n",
    "    flip_axis = 1\n",
    "\n",
    "    perturbed_images = np.empty_like(images)\n",
    "    for i in range(images.shape[0]):\n",
    "        perturbed_image = np.flip(images[i, 0, :, :], axis=flip_axis)\n",
    "        perturbed_images[i, 0, :, :] = perturbed_image\n",
    "    return perturbed_images\n",
    "\n",
    "\n",
    "flipped_images_array = test_flip(mnist_test_images)\n",
    "# Show example of the image after being flipped\n",
    "viz.show(flipped_images_array[1])\n",
    "\n",
    "flipped_images_tensor = loader.convert_mnist_numpy_to_tensor(flipped_images_array, mnist_test_labels)\n",
    "acc_mnist_flipped = tester.test(mnist_resnet_model, flipped_images_tensor)\n",
    "print(f'Test Accuracy MNIST (Flipped): {acc_mnist_flipped * 100:.2f}%')\n",
    "# Accuracy drops 50% but the image is obviously the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient Sign Method\n",
    "The ``fgsm_attack`` function takes three\n",
    "inputs, *image* is the original clean image ($x$), *epsilon* is\n",
    "the pixel-wise perturbation amount ($\\epsilon$), and *data_grad*\n",
    "is gradient of the loss w.r.t the input image\n",
    "($\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)$). The function\n",
    "then creates perturbed image as\n",
    "\n",
    "\\begin{align}\n",
    "\\text{perturbed\\_image} &= \\text{image} + \\epsilon \\cdot \\text{sign}(\\text{data\\_grad}) \\\\\n",
    "&x = x + \\epsilon \\cdot \\text{sign}(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\n",
    "\\end{align}\n",
    "\n",
    "Fast Gradient Sign Method uses $L^∞$ Norm Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fgsm(model, test_loader, epsilon):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    adv_examples = []\n",
    "    batch = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch += 1\n",
    "        print(f\"Batch: {batch}, Epsilon: {epsilon}, Correct: {correct}\")\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            label = label.unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            output, _ = model(image)\n",
    "\n",
    "            # print(outputs)\n",
    "\n",
    "            _, init_pred = torch.max(output.data, 1)\n",
    "\n",
    "            if not torch.equal(init_pred, label):\n",
    "                total +=1 \n",
    "                continue\n",
    "            \n",
    "            loss = F.nll_loss(output, label)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = image.grad.data\n",
    "            perturbed_data = fgsm_attack(image, epsilon, data_grad)\n",
    "\n",
    "            output_final, _ = model(perturbed_data)\n",
    "            _, final_pred = torch.max(output_final.data, 1)\n",
    "            if torch.equal(final_pred, label):\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            else:\n",
    "                # Save some adv examples for visualization later\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            total +=1 \n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {total} = {accuracy}\")\n",
    "    return accuracy, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .5]\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test_fgsm(mnist_resnet_model, test_loader_mnist, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .55, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(f\"{orig} -> {adv}\")\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFool\n",
    "The `deepfool_attack` function is designed to generate adversarial examples using the DeepFool algorithm. It iteratively picks the classified label closest to the true label that isn't the true label and then uses the gradient to move the image closer to that classification boundary while retaining image data as much as possible. It is considered a \"perfect\" algorithm as it terminates when the image misclassifies but we limit resources because this could take unreasonable amounts of time in theory. \n",
    "\n",
    "\\begin{array}{l}\n",
    "\\textbf{while } sign(f(x_i)) = sign(f(x_0)): \\\\\n",
    "\\hspace{2em} r_i = - \\frac{f(x_i)}{\\|\\nabla f(x_i)\\|_2^2} \\cdot \\nabla f(x) \\\\\n",
    "\\hspace{2em} x_{i+1} = x_i + r_i \\\\\n",
    "\\hspace{2em} i = i + 1\n",
    "\\end{array}\n",
    "\n",
    "Deepfool uses $L^2$ Norm Perturbations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deepfool(model, test_loader, overshoot=0.02):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    adv_examples = []\n",
    "    batch = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch += 1\n",
    "        print(f\"Batch: {batch}, Correct: {correct}\")\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            label = label.unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            output, _ = model(image)\n",
    "\n",
    "            # print(outputs)\n",
    "\n",
    "            _, init_pred = torch.max(output.data, 1)\n",
    "\n",
    "            if not torch.equal(init_pred, label):\n",
    "                total +=1 \n",
    "                continue\n",
    "            \n",
    "            perturbed_image, final_pred, r_total, iter = deepfool_attack(image, model, overshoot=0.02, max_iterations=100)\n",
    "            print(f\"Perturbed Iteration: {iter}\")\n",
    "            if torch.equal(final_pred, label):\n",
    "                correct += 1\n",
    "            total +=1 \n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy = {correct} / {total} = {accuracy}\")\n",
    "    return accuracy, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_deepfool(cifar_resnet_model, test_loader_cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Gradient Descent\n",
    "Projected Gradient Descent is an optimization algorithm that iteratively computes the gradient of the loss and perturbs the image in the direction of the loss gradient. This algorithm has unique hyperparameters. We have a value $α$ which controls the step size the image moves along the gradient, but we also have something known as the $ϵ$-ball. Around the image in the vector space exists a multi dimensional threshold where the image is still percievable to humans. We clip the perturbed output image such that it always exists within this epsilon ball, and we can expand or shrink the epsilon to tune results.\n",
    "\n",
    "\\begin{array}{l}\n",
    "\\textbf{while } t < \\text{ iterations:} \\\\\n",
    "\\hspace{2em} x_{t+1} = Clip(x_t + \\alpha \\cdot sign(\\nabla_x J(\\theta, x_t, y)), x, x + \\epsilon)\n",
    "\\end{array}\n",
    "\n",
    "PGA uses uses $L^∞$ Norm Perturbations and MUST be used against  classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pgd(model, test_loader, epsilon, alpha):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    adv_examples = []\n",
    "    batch = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch += 1\n",
    "        print(f\"Batch: {batch}, Epsilon: {epsilon}, Correct: {correct}\")\n",
    "        for image, label in zip(images, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            label = label.unsqueeze(0)\n",
    "            image.requires_grad = True\n",
    "            output, _ = model(image)\n",
    "\n",
    "            _, init_pred = torch.max(output.data, 1)\n",
    "\n",
    "            if not torch.equal(init_pred, label):\n",
    "                total +=1 \n",
    "                continue\n",
    "            \n",
    "            \n",
    "\n",
    "            output_final, perturbed_data = pgd_attack(image, model, init_pred, epsilon, alpha)\n",
    "            _, final_pred = torch.max(output_final.data, 1)\n",
    "            if torch.equal(final_pred, label):\n",
    "                correct += 1\n",
    "                if epsilon == 0 and len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            else:\n",
    "                # Save some adv examples for visualization later\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "            print(f\"{correct}/{total}\")\n",
    "            total +=1 \n",
    "            # break\n",
    "        # break\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {total} = {accuracy}\")\n",
    "    return accuracy, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Epsilon: 8, Correct: 0\n",
      "0/0\n",
      "1/1\n",
      "1/2\n",
      "2/3\n",
      "2/4\n",
      "2/5\n",
      "3/6\n",
      "3/7\n",
      "3/8\n",
      "4/9\n",
      "4/10\n",
      "4/11\n",
      "5/12\n",
      "6/13\n",
      "7/14\n",
      "7/15\n",
      "7/16\n",
      "8/17\n",
      "8/18\n",
      "9/19\n",
      "9/20\n",
      "9/21\n",
      "10/22\n",
      "10/23\n",
      "10/24\n",
      "10/25\n",
      "10/26\n",
      "11/27\n",
      "11/28\n",
      "11/29\n",
      "12/30\n",
      "13/31\n",
      "13/32\n",
      "13/33\n",
      "14/34\n",
      "15/35\n",
      "15/36\n",
      "16/37\n",
      "16/38\n",
      "16/39\n",
      "17/40\n",
      "17/41\n",
      "18/42\n",
      "18/43\n",
      "19/44\n",
      "20/45\n",
      "20/46\n",
      "21/47\n",
      "21/48\n",
      "22/49\n",
      "23/50\n",
      "23/51\n",
      "24/52\n",
      "25/53\n",
      "26/54\n",
      "27/55\n",
      "28/56\n",
      "28/57\n",
      "28/58\n",
      "28/59\n",
      "29/60\n",
      "29/61\n",
      "29/62\n",
      "29/63\n",
      "29/64\n",
      "29/65\n",
      "30/66\n",
      "30/67\n",
      "31/68\n",
      "32/69\n",
      "32/70\n",
      "33/71\n",
      "34/72\n",
      "34/73\n",
      "35/74\n",
      "36/75\n",
      "37/76\n",
      "38/77\n",
      "38/78\n",
      "38/79\n",
      "38/80\n",
      "38/81\n",
      "39/82\n",
      "40/83\n",
      "41/84\n",
      "42/85\n",
      "43/86\n",
      "43/87\n",
      "43/88\n",
      "43/89\n",
      "44/90\n",
      "44/91\n",
      "44/92\n",
      "44/93\n",
      "45/94\n",
      "45/95\n",
      "45/96\n",
      "46/97\n",
      "46/98\n",
      "47/99\n",
      "47/100\n",
      "48/101\n",
      "48/102\n",
      "49/103\n",
      "49/104\n",
      "50/105\n",
      "51/106\n",
      "51/107\n",
      "52/108\n",
      "53/109\n",
      "53/110\n",
      "53/111\n",
      "54/112\n",
      "55/113\n",
      "56/114\n",
      "56/115\n",
      "57/116\n",
      "58/117\n",
      "58/118\n",
      "58/119\n",
      "58/120\n",
      "58/121\n",
      "59/122\n",
      "60/123\n",
      "60/124\n",
      "60/125\n",
      "61/126\n",
      "62/127\n",
      "62/128\n",
      "63/129\n",
      "64/130\n",
      "64/131\n",
      "64/132\n",
      "64/133\n",
      "64/134\n",
      "64/135\n",
      "65/136\n",
      "65/137\n",
      "65/138\n",
      "65/139\n",
      "65/140\n",
      "66/141\n",
      "66/142\n",
      "67/143\n",
      "67/144\n",
      "68/145\n",
      "68/146\n",
      "69/147\n",
      "70/148\n",
      "70/149\n",
      "71/150\n",
      "71/151\n",
      "71/152\n",
      "72/153\n",
      "73/154\n",
      "74/155\n",
      "75/156\n",
      "76/157\n",
      "76/158\n",
      "76/159\n",
      "77/160\n",
      "77/161\n",
      "78/162\n",
      "79/163\n",
      "79/164\n",
      "80/165\n",
      "81/166\n",
      "81/167\n",
      "82/168\n",
      "83/169\n",
      "84/170\n",
      "84/171\n",
      "84/172\n",
      "84/173\n",
      "84/174\n",
      "84/175\n",
      "85/176\n",
      "85/177\n",
      "85/178\n",
      "85/179\n",
      "86/180\n",
      "87/181\n",
      "87/182\n",
      "88/183\n",
      "88/184\n",
      "88/185\n",
      "89/186\n",
      "89/187\n",
      "90/188\n",
      "91/189\n",
      "91/190\n",
      "91/191\n",
      "92/192\n",
      "92/193\n",
      "92/194\n",
      "92/195\n",
      "93/196\n",
      "94/197\n",
      "95/198\n",
      "95/199\n",
      "95/200\n",
      "96/201\n",
      "96/202\n",
      "96/203\n",
      "96/204\n",
      "97/205\n",
      "98/206\n",
      "99/207\n",
      "100/208\n",
      "100/209\n",
      "101/210\n",
      "101/211\n",
      "102/212\n",
      "103/213\n",
      "103/214\n",
      "104/215\n",
      "105/216\n",
      "105/217\n",
      "106/218\n",
      "106/219\n",
      "107/220\n",
      "107/221\n",
      "108/222\n",
      "108/223\n",
      "108/224\n",
      "108/225\n",
      "108/226\n",
      "108/227\n",
      "108/228\n",
      "108/229\n",
      "109/230\n",
      "110/231\n",
      "110/232\n",
      "111/233\n",
      "112/234\n",
      "112/235\n",
      "113/236\n",
      "114/237\n",
      "115/238\n",
      "116/239\n",
      "116/240\n",
      "116/241\n",
      "117/242\n",
      "118/243\n",
      "119/244\n",
      "119/245\n",
      "120/246\n",
      "120/248\n",
      "121/249\n",
      "121/250\n",
      "121/251\n",
      "122/252\n",
      "122/253\n",
      "122/254\n",
      "122/255\n",
      "Epsilon: 8\tTest Accuracy = 122 / 256 = 0.4765625\n",
      "0.4765625\n"
     ]
    }
   ],
   "source": [
    "accuracy, examples = test_pgd(mnist_resnet_model, test_loader_mnist, 8, 0.05 )\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 9, array([[0.        , 0.        , 0.        , 0.1       , 0.1       ,\n",
      "        0.1       , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.1       ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.        , 0.1       ,\n",
      "        0.        , 0.1       , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.1       , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.1       , 0.1       ,\n",
      "        0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.        , 0.        , 0.1       , 0.1       ,\n",
      "        0.1       , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.1       , 0.        ],\n",
      "       [0.        , 0.        , 0.1       , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.1       , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.        , 0.22941175, 0.6254902 , 0.5235294 , 0.49215686,\n",
      "        0.13529412, 0.04117648, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.1       ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.97058827, 0.8960784 , 0.8960784 , 0.8960784 ,\n",
      "        0.8960784 , 0.845098  , 0.6764706 , 0.8764706 , 0.6764706 ,\n",
      "        0.8764706 , 0.8764706 , 0.8764706 , 0.8764706 , 0.8764706 ,\n",
      "        0.56666666, 0.10392158, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.1       , 0.36274514, 0.5470588 , 0.18235296, 0.3470588 ,\n",
      "        0.5392157 , 0.79019606, 0.8960784 , 0.7823529 , 1.0960784 ,\n",
      "        1.0960784 , 0.8960784 , 1.0803921 , 0.99803925, 0.8960784 ,\n",
      "        0.8960784 , 0.4490196 , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.16666669, 0.35882357, 0.        , 0.36274514,\n",
      "        0.36274514, 0.36274514, 0.13137256, 0.        , 0.8254902 ,\n",
      "        0.8960784 , 0.31568626, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.1       ,\n",
      "        0.1       , 0.1       , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.1       , 0.        , 0.22549018, 0.89215684,\n",
      "        0.9196079 , 0.17058825, 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.1       , 0.1       , 0.        , 0.        ,\n",
      "        0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.1       , 0.        , 1.0137255 , 0.9       ,\n",
      "        0.42549023, 0.1       , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.1       , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.1       ,\n",
      "        0.        , 0.1       , 0.        , 0.1       , 0.1       ,\n",
      "        0.        , 0.1       , 0.6058824 , 0.8960784 , 0.8333333 ,\n",
      "        0.27254903, 0.1       , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.1       , 0.1       , 0.        , 0.1       , 0.1       ,\n",
      "        0.1       , 0.33137256, 0.87647057, 0.8960784 , 0.34313726,\n",
      "        0.        , 0.1       , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.1       , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.1       , 0.        , 0.1       , 0.1       ,\n",
      "        0.1       , 0.6215687 , 1.0960784 , 0.8333334 , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.70392156, 0.872549  , 0.327451  , 0.        ,\n",
      "        0.        , 0.1       , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.39411762, 0.8960784 , 0.81372553, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.1       , 0.        , 0.1       , 0.1       ,\n",
      "        0.1       , 0.        , 0.        , 0.        , 0.19411767,\n",
      "        0.8843137 , 0.84117645, 0.32352942, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.1       , 0.1       , 0.        , 0.1       , 0.1       ,\n",
      "        0.        , 0.1       , 0.1       , 0.1       , 0.        ,\n",
      "        0.1       , 0.1       , 0.        , 0.        , 0.76666665,\n",
      "        1.0960784 , 0.5509804 , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.1       , 0.        , 0.        , 0.6960784 , 1.0960784 ,\n",
      "        0.95882356, 0.03725491, 0.        , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.24901961, 1.0960784 , 1.0960784 ,\n",
      "        0.20196077, 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        ],\n",
      "       [0.1       , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.02156862, 0.77843136, 1.0960784 , 0.5509803 ,\n",
      "        0.10392157, 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.1       , 0.        ],\n",
      "       [0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.6215687 , 0.8960784 , 0.8960784 , 0.10392158,\n",
      "        0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.1       , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.3392157 , 1.0490196 , 0.8960784 , 0.8960784 , 0.10392158,\n",
      "        0.        , 0.1       , 0.1       , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.        , 0.1       , 0.1       ],\n",
      "       [0.1       , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.57450974, 1.0960784 , 0.8960784 , 0.7588235 , 0.25686276,\n",
      "        0.        , 0.        , 0.1       , 0.1       , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.1       , 0.1       ,\n",
      "        0.        , 0.1       , 0.        ],\n",
      "       [0.1       , 0.1       , 0.        , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.        , 0.        , 0.1       ,\n",
      "        0.57450974, 1.0960784 , 0.7117647 , 0.        , 0.        ,\n",
      "        0.1       , 0.        , 0.1       , 0.        , 0.        ,\n",
      "        0.        , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
      "        0.        , 0.1       , 0.        ],\n",
      "       [0.        , 0.        , 0.        , 0.1       , 0.1       ,\n",
      "        0.1       , 0.        , 0.1       , 0.        , 0.1       ,\n",
      "        0.1       , 0.        , 0.        , 0.1       , 0.        ,\n",
      "        0.1       , 0.1       , 0.1       , 0.        , 0.        ,\n",
      "        0.1       , 0.1       , 0.        , 0.1       , 0.1       ,\n",
      "        0.        , 0.        , 0.        ]], dtype=float32))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(examples[0])\n",
    "true_label = examples[0][0]\n",
    "false_label = examples[0][1]\n",
    "image_data = examples[0][2]  # Replace with your actual image data\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_data, cmap='gray')  # Assuming the image is grayscale, adjust cmap as needed\n",
    "\n",
    "# Set the title\n",
    "title = f\"{true_label} -> {false_label}\"\n",
    "plt.title(title)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
