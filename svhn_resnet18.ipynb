{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "svhn_directory = os.path.join(current_directory,\"SVHN\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model architecture\n",
    "To apply resnet18 to MNIST data we need to define our model architecture using pytorch. This task has been done by dozens of computer scientists before so finding pre-existing implementations of the architecture was quite easy. In fact, resnet18 is so popular for these tasks that there was an option to import the model altogether without redefining the model architecture but recreating it allows for opportunities for modification later and better interpretability of what the CNN is doing. \n",
    "\n",
    "Residual networks create a block for the input data and pass the original block of data combined with output from the previous layer into the next layer. This prevents loss of data integrity and vanishing gradients as the input gets propagated deeper into the network.\n",
    "\n",
    "<p align=\"center\"><img src=\"images/resnet18ex1.png\" alt=\"Diagram showing the skip block\" width=\"75%\"/> </br> This diagram shows the original block \"skipping\" a layer and being passed as input into the next layer </p>\n",
    "<p align=\"center\"><img src=\"images/resnet18ex2.png\" alt=\"Diagram showing the layers of Resnet18\" width=\"75%\"/></br> This diagram visualizes the internal layers specifications</p>\n",
    "\n",
    "We can see the intricacies of each layer. The data is first convoluted into a block which is passed to other layers. Resnet has 8 layers of convolutional filters before being pooled. In this example, the output is softmaxed but for our purposes we modify it to use a linear output to predict one of the 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture Sourced From: https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb\n",
    "# Resnet Paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
    "# https://www.researchgate.net/figure/ResNet-18-architecture-20-The-numbers-added-to-the-end-of-ResNet-represent-the_fig2_349241995\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Define the BasicBlock\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes * self.expansion, stride)  # Adjusted out_channels here\n",
    "        self.bn1 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes * self.expansion, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResnetSVHN(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResnetSVHN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetSVHN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the ResNet18 model\n",
    "NUM_CLASSES = 10  # Number of classes in MNIST\n",
    "resnet18_model = ResnetSVHN(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=NUM_CLASSES, grayscale=False)\n",
    "resnet18_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "SVHN data is saved in matlab .mat files. Scipy.io is used to open the tables and extract the images and labels. SVHN data comes as 32x32x3 images that are human viewable using PIL, but we need to convert these to pytorch image vectors of shape (# records, # channels, width, height). To accomplish this we flatten it into the same format as the CIFAR10 vectors, and then unflatten them into pytorch image format with our desired shape.\n",
    "\n",
    "The data is then converted to a tensor dataset and loaded for pytorch. To speed up computation we push the tensors to a GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(images):\n",
    "    \"\"\"\n",
    "    Flattens images back to a one hot vector format\n",
    "\n",
    "    Parameters:\n",
    "    - images: Numpy array representing the images with shape (num_images, height, width, channels)\n",
    "\n",
    "    Returns:\n",
    "    - Flat array where the first 1024 values represent the red channel, the next 1024 values represent the green channel,\n",
    "      and the last 1024 values represent the blue channel of pixels for each image.\n",
    "    Example:    \n",
    "        flat_array = flatten(train_images)\n",
    "    \"\"\"\n",
    "    num_images, _, _, _ = images.shape\n",
    "\n",
    "    # Reshape the images array to (num_images, 1024, 3)\n",
    "    reshaped_images = images.reshape((num_images, -1, 3))\n",
    "\n",
    "    # Split the reshaped array into red, green, and blue channels\n",
    "    red_channel = reshaped_images[:, :, 0]\n",
    "    green_channel = reshaped_images[:, :, 1]\n",
    "    blue_channel = reshaped_images[:, :, 2]\n",
    "\n",
    "    # Stack the three channels horizontally\n",
    "    stacked_channels = np.hstack([red_channel, green_channel, blue_channel])\n",
    "\n",
    "    return stacked_channels\n",
    "\n",
    "def load_svhn_data_mat(file_path):\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "\n",
    "    # Extract data and labels\n",
    "    images = mat_data['X']\n",
    "    labels = mat_data['y']\n",
    "\n",
    "    # Reshape the images to (num_samples, height, width, channels)\n",
    "    images = np.transpose(images, (3, 0, 1, 2))\n",
    "\n",
    "    # This replaces the label 10 with 0. For some reason the CUDA toolkit does not work if the labels are indexed 1-10 instead of 0-9.\n",
    "    labels[labels == 10] = 0\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_pytorch_images(data_array):\n",
    "    \"\"\"\n",
    "    Convert a single record into a pytorch image. Pytorch takes in a very specific input where each record is 3x32x32.\n",
    "\n",
    "    Parameters:\n",
    "    - One-hot vector the first 1024 values represent the red channel of pixels, the second 1024 values represent the green channel of pixels, and the last 1024 values represent the blue channel of pixels\n",
    "\n",
    "    Returns: \n",
    "    - a numpy array representing the image data in pytorch format\n",
    "    \"\"\"\n",
    "    num_images = data_array.shape[0]\n",
    "    image_size = 32\n",
    "\n",
    "    # Split the array into three parts\n",
    "    split_size = data_array.shape[1] // 3\n",
    "    red_channel = data_array[:, :split_size].reshape((num_images, 1, image_size, image_size))\n",
    "    green_channel = data_array[:, split_size:2*split_size].reshape((num_images, 1, image_size, image_size))\n",
    "    blue_channel = data_array[:, 2*split_size:].reshape((num_images, 1, image_size, image_size))\n",
    "\n",
    "    # Stack the channels along the second axis to get the final shape (num_images, 3, 32, 32)\n",
    "    return np.concatenate([red_channel, green_channel, blue_channel], axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from CSVs using pandas\n",
    "filepath = os.path.join(svhn_directory,\"train_32x32.mat\")\n",
    "train_data, train_labels = load_svhn_data_mat(filepath)\n",
    "\n",
    "filepath = os.path.join(svhn_directory,\"test_32x32.mat\")\n",
    "test_data, test_labels = load_svhn_data_mat(filepath)\n",
    "\n",
    "flattened_train_data = flatten(train_data)\n",
    "data_dict = {'pixel_{}'.format(i+1): flattened_train_data[:, i] for i in range(flattened_train_data.shape[1])}\n",
    "data_dict['label'] = [i[0] for i in train_labels]\n",
    "flattened_train_df = pd.DataFrame(data_dict)\n",
    "\n",
    "flattened_test_data = flatten(test_data)\n",
    "data_dict = {'pixel_{}'.format(i+1): flattened_test_data[:, i] for i in range(flattened_test_data.shape[1])}\n",
    "data_dict['label'] = [i[0] for i in test_labels]\n",
    "flattened_test_data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Extract labels and pixel values\n",
    "train_labels = flattened_train_df.iloc[:, -1].values\n",
    "train_images = flattened_train_df.iloc[:, :-1].values \n",
    "\n",
    "test_labels = flattened_test_data.iloc[:, -1].values\n",
    "test_images = flattened_test_data.iloc[:, :-1].values \n",
    "\n",
    "train_images = convert_to_pytorch_images(train_images)\n",
    "test_images = convert_to_pytorch_images(test_images)\n",
    "print(train_images.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images to [batch_size, 1, 32, 32]\n",
    "\n",
    "train_images_tensor = train_images_tensor.view(-1, 3, 32, 32)\n",
    "test_images_tensor = test_images_tensor.view(-1, 3, 32, 32)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=8, shuffle=False)\n",
    "# Move the data to the GPU\n",
    "train_images_tensor, train_labels_tensor = train_images_tensor.to(device), train_labels_tensor.to(device)\n",
    "test_images_tensor, test_labels_tensor = test_images_tensor.to(device), test_labels_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Training the model is straightforward: the resnet18 model is initialized with the learning rate and cross entropy loss function hyperparameters, and then training is run for a certain number of epochs by passing in image data into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1145], Loss: 0.9942\n",
      "Epoch [1/10], Step [200/1145], Loss: 0.7099\n",
      "Epoch [1/10], Step [300/1145], Loss: 0.6826\n",
      "Epoch [1/10], Step [400/1145], Loss: 0.5369\n",
      "Epoch [1/10], Step [500/1145], Loss: 0.4288\n",
      "Epoch [1/10], Step [600/1145], Loss: 0.4471\n",
      "Epoch [1/10], Step [700/1145], Loss: 0.4178\n",
      "Epoch [1/10], Step [800/1145], Loss: 0.2958\n",
      "Epoch [1/10], Step [900/1145], Loss: 0.4597\n",
      "Epoch [1/10], Step [1000/1145], Loss: 0.3149\n",
      "Epoch [1/10], Step [1100/1145], Loss: 0.3635\n",
      "Epoch [1/10], Training Accuracy: 88.4898\n",
      "Epoch [2/10], Step [100/1145], Loss: 0.1959\n",
      "Epoch [2/10], Step [200/1145], Loss: 0.2449\n",
      "Epoch [2/10], Step [300/1145], Loss: 0.4180\n",
      "Epoch [2/10], Step [400/1145], Loss: 0.4119\n",
      "Epoch [2/10], Step [500/1145], Loss: 0.5348\n",
      "Epoch [2/10], Step [600/1145], Loss: 0.4379\n",
      "Epoch [2/10], Step [700/1145], Loss: 0.3890\n",
      "Epoch [2/10], Step [800/1145], Loss: 0.4310\n",
      "Epoch [2/10], Step [900/1145], Loss: 0.3004\n",
      "Epoch [2/10], Step [1000/1145], Loss: 0.2393\n",
      "Epoch [2/10], Step [1100/1145], Loss: 0.4510\n",
      "Epoch [2/10], Training Accuracy: 90.5784\n",
      "Epoch [3/10], Step [100/1145], Loss: 0.2229\n",
      "Epoch [3/10], Step [200/1145], Loss: 0.1890\n",
      "Epoch [3/10], Step [300/1145], Loss: 0.2678\n",
      "Epoch [3/10], Step [400/1145], Loss: 0.3901\n",
      "Epoch [3/10], Step [500/1145], Loss: 0.3958\n",
      "Epoch [3/10], Step [600/1145], Loss: 0.3131\n",
      "Epoch [3/10], Step [700/1145], Loss: 0.2934\n",
      "Epoch [3/10], Step [800/1145], Loss: 0.2574\n",
      "Epoch [3/10], Step [900/1145], Loss: 0.2382\n",
      "Epoch [3/10], Step [1000/1145], Loss: 0.2597\n",
      "Epoch [3/10], Step [1100/1145], Loss: 0.2044\n",
      "Epoch [3/10], Training Accuracy: 92.1332\n",
      "Epoch [4/10], Step [100/1145], Loss: 0.3584\n",
      "Epoch [4/10], Step [200/1145], Loss: 0.4403\n",
      "Epoch [4/10], Step [300/1145], Loss: 0.2123\n",
      "Epoch [4/10], Step [400/1145], Loss: 0.2664\n",
      "Epoch [4/10], Step [500/1145], Loss: 0.4398\n",
      "Epoch [4/10], Step [600/1145], Loss: 0.1846\n",
      "Epoch [4/10], Step [700/1145], Loss: 0.1950\n",
      "Epoch [4/10], Step [800/1145], Loss: 0.3079\n",
      "Epoch [4/10], Step [900/1145], Loss: 0.3095\n",
      "Epoch [4/10], Step [1000/1145], Loss: 0.2454\n",
      "Epoch [4/10], Step [1100/1145], Loss: 0.1729\n",
      "Epoch [4/10], Training Accuracy: 93.4436\n",
      "Epoch [5/10], Step [100/1145], Loss: 0.2104\n",
      "Epoch [5/10], Step [200/1145], Loss: 0.5618\n",
      "Epoch [5/10], Step [300/1145], Loss: 0.3168\n",
      "Epoch [5/10], Step [400/1145], Loss: 0.1658\n",
      "Epoch [5/10], Step [500/1145], Loss: 0.2224\n",
      "Epoch [5/10], Step [600/1145], Loss: 0.2839\n",
      "Epoch [5/10], Step [700/1145], Loss: 0.0935\n",
      "Epoch [5/10], Step [800/1145], Loss: 0.2911\n",
      "Epoch [5/10], Step [900/1145], Loss: 0.2859\n",
      "Epoch [5/10], Step [1000/1145], Loss: 0.2056\n",
      "Epoch [5/10], Step [1100/1145], Loss: 0.1285\n",
      "Epoch [5/10], Training Accuracy: 94.5753\n",
      "Epoch [6/10], Step [100/1145], Loss: 0.1872\n",
      "Epoch [6/10], Step [200/1145], Loss: 0.1921\n",
      "Epoch [6/10], Step [300/1145], Loss: 0.1395\n",
      "Epoch [6/10], Step [400/1145], Loss: 0.1590\n",
      "Epoch [6/10], Step [500/1145], Loss: 0.1325\n",
      "Epoch [6/10], Step [600/1145], Loss: 0.1729\n",
      "Epoch [6/10], Step [700/1145], Loss: 0.1439\n",
      "Epoch [6/10], Step [800/1145], Loss: 0.3059\n",
      "Epoch [6/10], Step [900/1145], Loss: 0.2606\n",
      "Epoch [6/10], Step [1000/1145], Loss: 0.1410\n",
      "Epoch [6/10], Step [1100/1145], Loss: 0.2992\n",
      "Epoch [6/10], Training Accuracy: 95.0162\n",
      "Epoch [7/10], Step [100/1145], Loss: 0.0738\n",
      "Epoch [7/10], Step [200/1145], Loss: 0.3362\n",
      "Epoch [7/10], Step [300/1145], Loss: 0.5380\n",
      "Epoch [7/10], Step [400/1145], Loss: 0.0967\n",
      "Epoch [7/10], Step [500/1145], Loss: 0.0727\n",
      "Epoch [7/10], Step [600/1145], Loss: 0.1647\n",
      "Epoch [7/10], Step [700/1145], Loss: 0.0714\n",
      "Epoch [7/10], Step [800/1145], Loss: 0.1368\n",
      "Epoch [7/10], Step [900/1145], Loss: 0.1170\n",
      "Epoch [7/10], Step [1000/1145], Loss: 0.2485\n",
      "Epoch [7/10], Step [1100/1145], Loss: 0.1449\n",
      "Epoch [7/10], Training Accuracy: 95.8775\n",
      "Epoch [8/10], Step [100/1145], Loss: 0.1371\n",
      "Epoch [8/10], Step [200/1145], Loss: 0.2309\n",
      "Epoch [8/10], Step [300/1145], Loss: 0.0571\n",
      "Epoch [8/10], Step [400/1145], Loss: 0.0772\n",
      "Epoch [8/10], Step [500/1145], Loss: 0.2400\n",
      "Epoch [8/10], Step [600/1145], Loss: 0.0699\n",
      "Epoch [8/10], Step [700/1145], Loss: 0.1762\n",
      "Epoch [8/10], Step [800/1145], Loss: 0.1011\n",
      "Epoch [8/10], Step [900/1145], Loss: 0.2469\n",
      "Epoch [8/10], Step [1000/1145], Loss: 0.0693\n",
      "Epoch [8/10], Step [1100/1145], Loss: 0.1385\n",
      "Epoch [8/10], Training Accuracy: 96.6652\n",
      "Epoch [9/10], Step [100/1145], Loss: 0.1718\n",
      "Epoch [9/10], Step [200/1145], Loss: 0.2908\n",
      "Epoch [9/10], Step [300/1145], Loss: 0.2049\n",
      "Epoch [9/10], Step [400/1145], Loss: 0.1514\n",
      "Epoch [9/10], Step [500/1145], Loss: 0.0366\n",
      "Epoch [9/10], Step [600/1145], Loss: 0.0713\n",
      "Epoch [9/10], Step [700/1145], Loss: 0.0502\n",
      "Epoch [9/10], Step [800/1145], Loss: 0.2069\n",
      "Epoch [9/10], Step [900/1145], Loss: 0.1004\n",
      "Epoch [9/10], Step [1000/1145], Loss: 0.2938\n",
      "Epoch [9/10], Step [1100/1145], Loss: 0.0964\n",
      "Epoch [9/10], Training Accuracy: 96.6843\n",
      "Epoch [10/10], Step [100/1145], Loss: 0.0550\n",
      "Epoch [10/10], Step [200/1145], Loss: 0.0401\n",
      "Epoch [10/10], Step [300/1145], Loss: 0.0796\n",
      "Epoch [10/10], Step [400/1145], Loss: 0.2383\n",
      "Epoch [10/10], Step [500/1145], Loss: 0.0244\n",
      "Epoch [10/10], Step [600/1145], Loss: 0.1011\n",
      "Epoch [10/10], Step [700/1145], Loss: 0.1058\n",
      "Epoch [10/10], Step [800/1145], Loss: 0.0672\n",
      "Epoch [10/10], Step [900/1145], Loss: 0.0786\n",
      "Epoch [10/10], Step [1000/1145], Loss: 0.0919\n",
      "Epoch [10/10], Step [1100/1145], Loss: 0.0838\n",
      "Epoch [10/10], Training Accuracy: 97.3204\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18_model.parameters(), lr=0.001)\n",
    "\n",
    "# Compute Train Accuracy\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for _, (features, targets) in enumerate(data_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        _, class_probabilities = model(features)\n",
    "        _, predicted_labels = torch.max(class_probabilities, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum().item()\n",
    "    return correct_pred / num_examples * 100\n",
    "\n",
    "# While training run nvidia-smi in the terminal to check gpu tasks \n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to the GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs, _ = resnet18_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    training_accuracy = compute_accuracy(resnet18_model, train_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Accuracy: {training_accuracy:.4f}')\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18_model.state_dict(), 'resnet18_svhn_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "Using the test tensor we initialized above, we can set the model into evaluation mode and then record how accurate it is at making predictions. SVHN in the resnet18 model achieved ~92% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.94%\n"
     ]
    }
   ],
   "source": [
    "# Move the test data to the GPU\n",
    "test_images_tensor, test_labels_tensor = test_images_tensor.to(device), test_labels_tensor.to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "resnet18_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs, _ = resnet18_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
