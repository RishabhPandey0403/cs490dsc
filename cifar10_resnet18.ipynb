{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "cifar_directory = os.path.join(current_directory,\"CIFAR-10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model architecture\n",
    "To apply resnet18 to MNIST data we need to define our model architecture using pytorch. This task has been done by dozens of computer scientists before so finding pre-existing implementations of the architecture was quite easy. In fact, resnet18 is so popular for these tasks that there was an option to import the model altogether without redefining the model architecture but recreating it allows for opportunities for modification later and better interpretability of what the CNN is doing. \n",
    "\n",
    "Residual networks create a block for the input data and pass the original block of data combined with output from the previous layer into the next layer. This prevents loss of data integrity and vanishing gradients as the input gets propagated deeper into the network.\n",
    "\n",
    "<p align=\"center\"><img src=\"images/resnet18ex1.png\" alt=\"Diagram showing the skip block\" width=\"75%\"/> </br> This diagram shows the original block \"skipping\" a layer and being passed as input into the next layer </p>\n",
    "<p align=\"center\"><img src=\"images/resnet18ex2.png\" alt=\"Diagram showing the layers of Resnet18\" width=\"75%\"/></br> This diagram visualizes the internal layers specifications</p>\n",
    "\n",
    "We can see the intricacies of each layer. The data is first convoluted into a block which is passed to other layers. Resnet has 8 layers of convolutional filters before being pooled. In this example, the output is softmaxed but for our purposes we modify it to use a linear output to predict one of the 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture Sourced From: https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb\n",
    "# Resnet Paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
    "# https://www.researchgate.net/figure/ResNet-18-architecture-20-The-numbers-added-to-the-end-of-ResNet-represent-the_fig2_349241995\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Define the BasicBlock\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes * self.expansion, stride)  # Adjusted out_channels here\n",
    "        self.bn1 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes * self.expansion, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResNetCIFAR(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNetCIFAR, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetCIFAR(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the ResNet18 model\n",
    "NUM_CLASSES = 10  # Number of classes in MNIST\n",
    "resnet18_model = ResNetCIFAR(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=NUM_CLASSES, grayscale=False)\n",
    "resnet18_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "CIFAR10 is saved in batches of bytecode and needs to be reassembled. The bytecode is opened using pickle and then appended into a numpy array. Unfortunately, the neural network architecture we have does not use one-hot encoding vectors (due to being a CNN and not a vanilla NN) so the data must be unflattened into a pytorch image format with the shape (# records, # channels, width, height). \n",
    "\n",
    "The data is then converted to a tensor dataset and loaded for pytorch. To speed up computation we push the tensors to a GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "# Helper function which concatenate the data from the 5 files into one large file\n",
    "def load_cifar10_data(data_dir):\n",
    "    train_batches = []\n",
    "    for i in range(1, 6):\n",
    "        file_path = f\"{data_dir}/data_batch_{i}\"\n",
    "        train_batches.append(load_cifar10_batch(file_path))\n",
    "\n",
    "    test_batch = load_cifar10_batch(f\"{data_dir}/test_batch\")\n",
    "\n",
    "    # Concatenate training batches to get the full training dataset\n",
    "    train_data = np.concatenate([batch[b'data'] for batch in train_batches])\n",
    "    train_labels = np.concatenate([batch[b'labels'] for batch in train_batches])\n",
    "\n",
    "    # Extract test data and labels\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def convert_to_pytorch_images(data_array):\n",
    "    \"\"\"\n",
    "    Convert a single record into a pytorch image. Pytorch takes in a very specific input where each record is 3x32x32.\n",
    "\n",
    "    Parameters:\n",
    "    - One-hot vector the first 1024 values represent the red channel of pixels, the second 1024 values represent the green channel of pixels, and the last 1024 values represent the blue channel of pixels\n",
    "\n",
    "    Returns: \n",
    "    - a numpy array representing the image data in pytorch format\n",
    "    \"\"\"\n",
    "    num_images = data_array.shape[0]\n",
    "    image_size = 32\n",
    "\n",
    "    # Split the array into three parts\n",
    "    split_size = data_array.shape[1] // 3\n",
    "    red_channel = data_array[:, :split_size].reshape((num_images, 1, image_size, image_size))\n",
    "    green_channel = data_array[:, split_size:2*split_size].reshape((num_images, 1, image_size, image_size))\n",
    "    blue_channel = data_array[:, 2*split_size:].reshape((num_images, 1, image_size, image_size))\n",
    "\n",
    "    # Stack the channels along the second axis to get the final shape (num_images, 3, 32, 32)\n",
    "    return np.concatenate([red_channel, green_channel, blue_channel], axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from CSVs using pandas\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar_directory)\n",
    "\n",
    "# Append the labels\n",
    "columns = [f\"pixel_{i+1}\" for i in range(train_data.shape[1])]\n",
    "cifar_train = pd.DataFrame(train_data, columns=columns)\n",
    "cifar_train['label'] = train_labels\n",
    "\n",
    "cifar_test = pd.DataFrame(test_data, columns=columns)\n",
    "cifar_test['label'] = test_labels\n",
    "\n",
    "# Extract labels and pixel values\n",
    "train_labels = cifar_train.iloc[:, -1].values\n",
    "train_images = cifar_train.iloc[:, :-1].values \n",
    "\n",
    "test_labels = cifar_test.iloc[:, -1].values\n",
    "test_images = cifar_test.iloc[:, :-1].values \n",
    "\n",
    "train_images = convert_to_pytorch_images(train_images)\n",
    "test_images = convert_to_pytorch_images(test_images)\n",
    "print(train_images.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the images to [batch_size, 1, 32, 32]\n",
    "\n",
    "train_images_tensor = train_images_tensor.view(-1, 3, 32, 32)\n",
    "test_images_tensor = test_images_tensor.view(-1, 3, 32, 32)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Move the data to the GPU\n",
    "train_images_tensor, train_labels_tensor = train_images_tensor.to(device), train_labels_tensor.to(device)\n",
    "test_images_tensor, test_labels_tensor = test_images_tensor.to(device), test_labels_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Training the model is straightforward: the resnet18 model is initialized with the learning rate and cross entropy loss function hyperparameters, and then training is run for a certain number of epochs by passing in image data into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [100/782], Loss: 1.6466\n",
      "Epoch [1/25], Step [200/782], Loss: 1.5781\n",
      "Epoch [1/25], Step [300/782], Loss: 1.4247\n",
      "Epoch [1/25], Step [400/782], Loss: 1.2129\n",
      "Epoch [1/25], Step [500/782], Loss: 1.1511\n",
      "Epoch [1/25], Step [600/782], Loss: 1.2006\n",
      "Epoch [1/25], Step [700/782], Loss: 1.2726\n",
      "Epoch [2/25], Step [100/782], Loss: 1.0976\n",
      "Epoch [2/25], Step [200/782], Loss: 1.1560\n",
      "Epoch [2/25], Step [300/782], Loss: 0.9266\n",
      "Epoch [2/25], Step [400/782], Loss: 1.1668\n",
      "Epoch [2/25], Step [500/782], Loss: 0.9877\n",
      "Epoch [2/25], Step [600/782], Loss: 0.7840\n",
      "Epoch [2/25], Step [700/782], Loss: 0.9437\n",
      "Epoch [3/25], Step [100/782], Loss: 0.6503\n",
      "Epoch [3/25], Step [200/782], Loss: 1.2337\n",
      "Epoch [3/25], Step [300/782], Loss: 0.7613\n",
      "Epoch [3/25], Step [400/782], Loss: 1.1070\n",
      "Epoch [3/25], Step [500/782], Loss: 0.8718\n",
      "Epoch [3/25], Step [600/782], Loss: 0.9632\n",
      "Epoch [3/25], Step [700/782], Loss: 1.1466\n",
      "Epoch [4/25], Step [100/782], Loss: 0.6351\n",
      "Epoch [4/25], Step [200/782], Loss: 0.6591\n",
      "Epoch [4/25], Step [300/782], Loss: 0.6912\n",
      "Epoch [4/25], Step [400/782], Loss: 0.7909\n",
      "Epoch [4/25], Step [500/782], Loss: 0.5402\n",
      "Epoch [4/25], Step [600/782], Loss: 0.7535\n",
      "Epoch [4/25], Step [700/782], Loss: 0.8845\n",
      "Epoch [5/25], Step [100/782], Loss: 0.5928\n",
      "Epoch [5/25], Step [200/782], Loss: 0.7195\n",
      "Epoch [5/25], Step [300/782], Loss: 0.5617\n",
      "Epoch [5/25], Step [400/782], Loss: 0.6155\n",
      "Epoch [5/25], Step [500/782], Loss: 0.5059\n",
      "Epoch [5/25], Step [600/782], Loss: 0.8708\n",
      "Epoch [5/25], Step [700/782], Loss: 0.5927\n",
      "Epoch [6/25], Step [100/782], Loss: 0.5406\n",
      "Epoch [6/25], Step [200/782], Loss: 0.2843\n",
      "Epoch [6/25], Step [300/782], Loss: 0.6687\n",
      "Epoch [6/25], Step [400/782], Loss: 0.4242\n",
      "Epoch [6/25], Step [500/782], Loss: 0.4709\n",
      "Epoch [6/25], Step [600/782], Loss: 0.3210\n",
      "Epoch [6/25], Step [700/782], Loss: 0.5275\n",
      "Epoch [7/25], Step [100/782], Loss: 0.3647\n",
      "Epoch [7/25], Step [200/782], Loss: 0.5225\n",
      "Epoch [7/25], Step [300/782], Loss: 0.4824\n",
      "Epoch [7/25], Step [400/782], Loss: 0.4251\n",
      "Epoch [7/25], Step [500/782], Loss: 0.5769\n",
      "Epoch [7/25], Step [600/782], Loss: 0.5336\n",
      "Epoch [7/25], Step [700/782], Loss: 0.3740\n",
      "Epoch [8/25], Step [100/782], Loss: 0.3208\n",
      "Epoch [8/25], Step [200/782], Loss: 0.2948\n",
      "Epoch [8/25], Step [300/782], Loss: 0.3387\n",
      "Epoch [8/25], Step [400/782], Loss: 0.4141\n",
      "Epoch [8/25], Step [500/782], Loss: 0.4021\n",
      "Epoch [8/25], Step [600/782], Loss: 0.3819\n",
      "Epoch [8/25], Step [700/782], Loss: 0.4121\n",
      "Epoch [9/25], Step [100/782], Loss: 0.0997\n",
      "Epoch [9/25], Step [200/782], Loss: 0.1899\n",
      "Epoch [9/25], Step [300/782], Loss: 0.1538\n",
      "Epoch [9/25], Step [400/782], Loss: 0.1603\n",
      "Epoch [9/25], Step [500/782], Loss: 0.2010\n",
      "Epoch [9/25], Step [600/782], Loss: 0.2334\n",
      "Epoch [9/25], Step [700/782], Loss: 0.2240\n",
      "Epoch [10/25], Step [100/782], Loss: 0.2216\n",
      "Epoch [10/25], Step [200/782], Loss: 0.1765\n",
      "Epoch [10/25], Step [300/782], Loss: 0.2028\n",
      "Epoch [10/25], Step [400/782], Loss: 0.2067\n",
      "Epoch [10/25], Step [500/782], Loss: 0.2866\n",
      "Epoch [10/25], Step [600/782], Loss: 0.2741\n",
      "Epoch [10/25], Step [700/782], Loss: 0.1623\n",
      "Epoch [11/25], Step [100/782], Loss: 0.1253\n",
      "Epoch [11/25], Step [200/782], Loss: 0.0952\n",
      "Epoch [11/25], Step [300/782], Loss: 0.1134\n",
      "Epoch [11/25], Step [400/782], Loss: 0.2079\n",
      "Epoch [11/25], Step [500/782], Loss: 0.2397\n",
      "Epoch [11/25], Step [600/782], Loss: 0.1987\n",
      "Epoch [11/25], Step [700/782], Loss: 0.1463\n",
      "Epoch [12/25], Step [100/782], Loss: 0.1531\n",
      "Epoch [12/25], Step [200/782], Loss: 0.2531\n",
      "Epoch [12/25], Step [300/782], Loss: 0.2028\n",
      "Epoch [12/25], Step [400/782], Loss: 0.0948\n",
      "Epoch [12/25], Step [500/782], Loss: 0.2229\n",
      "Epoch [12/25], Step [600/782], Loss: 0.1565\n",
      "Epoch [12/25], Step [700/782], Loss: 0.2234\n",
      "Epoch [13/25], Step [100/782], Loss: 0.1559\n",
      "Epoch [13/25], Step [200/782], Loss: 0.0897\n",
      "Epoch [13/25], Step [300/782], Loss: 0.0428\n",
      "Epoch [13/25], Step [400/782], Loss: 0.0870\n",
      "Epoch [13/25], Step [500/782], Loss: 0.0745\n",
      "Epoch [13/25], Step [600/782], Loss: 0.1658\n",
      "Epoch [13/25], Step [700/782], Loss: 0.0576\n",
      "Epoch [14/25], Step [100/782], Loss: 0.1980\n",
      "Epoch [14/25], Step [200/782], Loss: 0.0320\n",
      "Epoch [14/25], Step [300/782], Loss: 0.2157\n",
      "Epoch [14/25], Step [400/782], Loss: 0.0536\n",
      "Epoch [14/25], Step [500/782], Loss: 0.3575\n",
      "Epoch [14/25], Step [600/782], Loss: 0.1671\n",
      "Epoch [14/25], Step [700/782], Loss: 0.0868\n",
      "Epoch [15/25], Step [100/782], Loss: 0.1397\n",
      "Epoch [15/25], Step [200/782], Loss: 0.0642\n",
      "Epoch [15/25], Step [300/782], Loss: 0.0205\n",
      "Epoch [15/25], Step [400/782], Loss: 0.0456\n",
      "Epoch [15/25], Step [500/782], Loss: 0.1152\n",
      "Epoch [15/25], Step [600/782], Loss: 0.1057\n",
      "Epoch [15/25], Step [700/782], Loss: 0.2611\n",
      "Epoch [16/25], Step [100/782], Loss: 0.0661\n",
      "Epoch [16/25], Step [200/782], Loss: 0.0480\n",
      "Epoch [16/25], Step [300/782], Loss: 0.0923\n",
      "Epoch [16/25], Step [400/782], Loss: 0.0788\n",
      "Epoch [16/25], Step [500/782], Loss: 0.1070\n",
      "Epoch [16/25], Step [600/782], Loss: 0.0172\n",
      "Epoch [16/25], Step [700/782], Loss: 0.1022\n",
      "Epoch [17/25], Step [100/782], Loss: 0.1209\n",
      "Epoch [17/25], Step [200/782], Loss: 0.0374\n",
      "Epoch [17/25], Step [300/782], Loss: 0.1354\n",
      "Epoch [17/25], Step [400/782], Loss: 0.0120\n",
      "Epoch [17/25], Step [500/782], Loss: 0.0094\n",
      "Epoch [17/25], Step [600/782], Loss: 0.0539\n",
      "Epoch [17/25], Step [700/782], Loss: 0.1276\n",
      "Epoch [18/25], Step [100/782], Loss: 0.1135\n",
      "Epoch [18/25], Step [200/782], Loss: 0.0580\n",
      "Epoch [18/25], Step [300/782], Loss: 0.1800\n",
      "Epoch [18/25], Step [400/782], Loss: 0.1436\n",
      "Epoch [18/25], Step [500/782], Loss: 0.0866\n",
      "Epoch [18/25], Step [600/782], Loss: 0.0866\n",
      "Epoch [18/25], Step [700/782], Loss: 0.0337\n",
      "Epoch [19/25], Step [100/782], Loss: 0.0101\n",
      "Epoch [19/25], Step [200/782], Loss: 0.2286\n",
      "Epoch [19/25], Step [300/782], Loss: 0.1383\n",
      "Epoch [19/25], Step [400/782], Loss: 0.0233\n",
      "Epoch [19/25], Step [500/782], Loss: 0.0540\n",
      "Epoch [19/25], Step [600/782], Loss: 0.0679\n",
      "Epoch [19/25], Step [700/782], Loss: 0.0390\n",
      "Epoch [20/25], Step [100/782], Loss: 0.0901\n",
      "Epoch [20/25], Step [200/782], Loss: 0.0818\n",
      "Epoch [20/25], Step [300/782], Loss: 0.1206\n",
      "Epoch [20/25], Step [400/782], Loss: 0.0100\n",
      "Epoch [20/25], Step [500/782], Loss: 0.0186\n",
      "Epoch [20/25], Step [600/782], Loss: 0.1170\n",
      "Epoch [20/25], Step [700/782], Loss: 0.0866\n",
      "Epoch [21/25], Step [100/782], Loss: 0.0314\n",
      "Epoch [21/25], Step [200/782], Loss: 0.1314\n",
      "Epoch [21/25], Step [300/782], Loss: 0.0928\n",
      "Epoch [21/25], Step [400/782], Loss: 0.0647\n",
      "Epoch [21/25], Step [500/782], Loss: 0.0142\n",
      "Epoch [21/25], Step [600/782], Loss: 0.0266\n",
      "Epoch [21/25], Step [700/782], Loss: 0.0244\n",
      "Epoch [22/25], Step [100/782], Loss: 0.0481\n",
      "Epoch [22/25], Step [200/782], Loss: 0.0204\n",
      "Epoch [22/25], Step [300/782], Loss: 0.0122\n",
      "Epoch [22/25], Step [400/782], Loss: 0.0181\n",
      "Epoch [22/25], Step [500/782], Loss: 0.2008\n",
      "Epoch [22/25], Step [600/782], Loss: 0.0243\n",
      "Epoch [22/25], Step [700/782], Loss: 0.0750\n",
      "Epoch [23/25], Step [100/782], Loss: 0.0149\n",
      "Epoch [23/25], Step [200/782], Loss: 0.0486\n",
      "Epoch [23/25], Step [300/782], Loss: 0.0567\n",
      "Epoch [23/25], Step [400/782], Loss: 0.1380\n",
      "Epoch [23/25], Step [500/782], Loss: 0.0474\n",
      "Epoch [23/25], Step [600/782], Loss: 0.0959\n",
      "Epoch [23/25], Step [700/782], Loss: 0.0891\n",
      "Epoch [24/25], Step [100/782], Loss: 0.0415\n",
      "Epoch [24/25], Step [200/782], Loss: 0.1923\n",
      "Epoch [24/25], Step [300/782], Loss: 0.0199\n",
      "Epoch [24/25], Step [400/782], Loss: 0.2189\n",
      "Epoch [24/25], Step [500/782], Loss: 0.0754\n",
      "Epoch [24/25], Step [600/782], Loss: 0.1013\n",
      "Epoch [24/25], Step [700/782], Loss: 0.1093\n",
      "Epoch [25/25], Step [100/782], Loss: 0.0545\n",
      "Epoch [25/25], Step [200/782], Loss: 0.0052\n",
      "Epoch [25/25], Step [300/782], Loss: 0.1095\n",
      "Epoch [25/25], Step [400/782], Loss: 0.0383\n",
      "Epoch [25/25], Step [500/782], Loss: 0.0975\n",
      "Epoch [25/25], Step [600/782], Loss: 0.0187\n",
      "Epoch [25/25], Step [700/782], Loss: 0.0237\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18_model.parameters(), lr=0.001)\n",
    "\n",
    "# While training run nvidia-smi in the terminal to check gpu tasks \n",
    "\n",
    "# Training the model\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to the GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs, _ = resnet18_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18_model.state_dict(), 'resnet18_cifar_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "Using the test tensor we initialized above, we can set the model into evaluation mode and then record how accurate it is at making predictions. CIFAR10 in the resnet18 model achieved ~75% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.09%\n"
     ]
    }
   ],
   "source": [
    "# Move the test data to the GPU\n",
    "test_images_tensor, test_labels_tensor = test_images_tensor.to(device), test_labels_tensor.to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "resnet18_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs, _ = resnet18_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
